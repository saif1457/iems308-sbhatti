{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## IEMS 308 - Dillard's SKU Analysis\n",
    "\n",
    "The Dillard's SKU dataset can be [downloaded here](https://www.dropbox.com/s/5o0v3uoakkqe8u6/Dillards%20POS.7z?dl=0).\n",
    "\n",
    "The final project report can be [found here](www.google.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import random\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from tabulate import tabulate\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 9,
        "hidden": false,
        "row": 4,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Because this dataset is so large, we need to in some \"clever way, select a subset of data\". \n",
    "\n",
    "To do this: \n",
    "1. Load the `strinfo` dataset and take a random sample of `stores` to pick. \n",
    "    1. This drastically reduces the datasize required to parse.\n",
    "    2. Store 9404 is in Oklahoma City, OK - this is chosen store to develop plantograms for.\n",
    "   \n",
    "2. Load just the barebones of the `transact` table, dropping out the unnecessary columns and keeping only `sku`,`store`, `register`,`trannum`, `seq`, `saledate`, `stype`.\n",
    "    1. One good method is to use `chunksize` because otherwise Python will not let the file be loaded into memory. It seems 1,000,000 is a good chunksize. \n",
    "    2. As each chunk is collected, only retain the relevant store transaction data.\n",
    "    3. The chunks can be concatenated into a fully-functional dataframe.\n",
    "3. Now that the barebones have been made into a functional dataframe, drop all columns except `sku`, `trannum` and `stype`.\n",
    "    1. It's important to ensure these are integer values and all whitespace is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "str_cols = ['store', 'city', 'state', 'zip']\n",
    "strinfo = pd.read_csv('dillard/strinfo.csv', sep=',',header=None, usecols=range(0, 4), names=str_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# stores = random.sample(strinfo.store.tolist(), 1)\n",
    "# strinfo[strinfo['state'] == 'TX'].head(10)\n",
    "\n",
    "stores=[9404] #focus on Oklahoma City, OK\n",
    "strinfo[strinfo['store'].isin(stores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#transcation data\n",
    "transaction_col = ['sku', 'store', 'register', 'trannum', 'seq', 'saledate', 'stype']\n",
    "transactions_iter = pd.read_csv('dillard/trnsact.csv', sep=',', header=None,\n",
    "                            chunksize=1000000, usecols=range(0, 7), names=transaction_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for trans_iter in transactions_iter:\n",
    "    cleaned = trans_iter[(trans_iter.store.isin(stores))]\n",
    "    temp_list.append(cleaned)\n",
    "transactions_compiled = pd.concat(temp_list) # this will be the entire (subsetted) dataframe of trnsact.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Keep only `stype=P` since only purchases contribute to revenue.\n",
    "2. Removing duplicate records.\n",
    "3. Intra-transaction SKUs are represented as just 1 instance instead of many.\n",
    "4. SKUs that were represented less than 5 instances across transaction table were removed due to lack of prevalence.\n",
    "5. All remaining SKUs were one-hot encoded so that they appeared as binary indicators per transaction row to signify if a particular SKU was purchased in that particular transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_compiled.groupby('store').describe()\n",
    "transactions_compiled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transactions_compiled.shape)\n",
    "#stores = [2409, 1904, 6700, 3202, 8702]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all columns except sku, store trannum, stype\n",
    "transactions_compiled = transactions_compiled[['sku','store','trannum','stype']].copy()\n",
    "transactions_compiled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_compiled.groupby('store').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain subset of dataframe that contains only purchases\n",
    "transactions_compiled = transactions_compiled[transactions_compiled['stype'].str.contains('P')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we only have purchases, drop the col\n",
    "transactions_compiled = transactions_compiled.drop('stype',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that the data is the right type\n",
    "transactions_compiled.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_counts_s = transactions_compiled['sku'].value_counts() #this is a pandas series\n",
    "type(sku_counts_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_counts_df = sku_counts_s.to_frame() #convert series to dataframe\n",
    "sku_counts_df = sku_counts_df.reset_index()\n",
    "sku_counts_df.columns = ['sku','count']\n",
    "print(type(sku_counts_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_counts = pd.merge(transactions_compiled,sku_counts_df, on=['sku']) #merge content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_noduplicates = transaction_counts.drop_duplicates() #drop any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of tran_noduplicates where all counts are 5 or higher\n",
    "transactions_clean = transactions_noduplicates[transactions_noduplicates['count'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_clean.head(5)\n",
    "\n",
    "# #One hot encode the transactions so that each row is a unique transaction and each column denotes \n",
    "# #the presence of an SKU (1) or the lack of an SKU (0) for the given transaction\n",
    "# df_biloxi_encoded = pd.get_dummies(df_biloxi_before_encoding,columns=['sku'])\n",
    "# del(df_biloxi_encoded['count'])\n",
    "# df_biloxi_reencoded = df_biloxi_encoded.groupby('trannum').sum()\n",
    "\n",
    "# #Run the apriori algorithm\n",
    "# frequent_itemsets = apriori(df_biloxi_reencoded,min_support=0.103, use_colnames=True)\n",
    "# rules = association_rules(frequent_itemsets,metric=\"lift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My jupyter kernel keeps dying at this step, so I'll save the dataframe to csv just to be sure we can keep making progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_clean.to_csv('transactions_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_clean=pd.read_csv('transactions_clean.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_dummy = pd.get_dummies(transactions_clean,columns=['sku'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_dummy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(transactions_dummy['count'])\n",
    "del(transactions_dummy['store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_twice_encoded = transactions_dummy.groupby('trannum').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_twice_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_dummy_df = pd.concat([transactions_clean,transactions_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_dummy_df = transactions_dummy_df.drop('sku',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_dummy_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(transactions_dummy_df['count'])\n",
    "# del(transactions_dummy_df['store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_twice_encoded = transactions_dummy_df.groupby('trannum').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_twice_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Associative Rule Analysis\n",
    "\n",
    "1. Run Algorithm.\n",
    "2. Compute `support`, `lift`, `confidence` metrics. \n",
    "    1. Only keep if `support` > 0.25 AND `confidence` > 0.5 AND `lift` > 3\n",
    "        1. OR\n",
    "    2. IF`support` > 0.6 \n",
    "        2. OR\n",
    "    3. `confidence` > 0.75 \n",
    "        3. OR \n",
    "    4. `lift` > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the apriori algorithm\n",
    "frequent_itemsets = apriori(t_twice_encoded,min_support=0.103, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets,metric=\"lift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate rules with antecedents having over 2 items\n",
    "rules_remainder = rules[rules['antecedents'].map(len)<=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_rules = rules_remainder[((rules_remainder['support']>.25) & (rules_remainder['confidence']>.5) & (rules_remainder['lift']>3)) | \\\n",
    "             ((rules_remainder['support']>.6) | (rules_remainder['confidence']>.75) | (rules_remainder['lift']>4))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_rules = useful_rules.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the total number of unique antecedent SKUs in useful_rules\n",
    "antecedents = []\n",
    "for i in range(0,np.shape(useful_rules)[0]):\n",
    "    antecedents.append(list(useful_rules['antecedents'].iloc[i])[0])\n",
    "    if len(useful_rules['antecedents'].iloc[i])==2:\n",
    "        antecedents.append(list(useful_rules['antecedents'].iloc[i])[1])\n",
    "antecedents = np.asarray(antecedents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add most frequently bought SKUs to the list\n",
    "frequent_counts = transactions_clean.sort_values(by='count',ascending=False).drop_duplicates(subset='sku')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antecedent_integers = []\n",
    "for num in antecedents:\n",
    "    antecedent_integers.append(int(num[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0    \n",
    "total_skus = np.unique(antecedent_integers)\n",
    "while len(total_skus)<100:\n",
    "    if frequent_counts['sku'].iloc[cnt] not in total_skus:\n",
    "        total_skus = np.append(total_skus,frequent_counts['sku'].iloc[cnt])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output final SKUs to move\n",
    "print(total_skus)"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
