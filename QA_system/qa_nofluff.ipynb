{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEMS 308 - Question Answer System (w/o fluff and errors)\n",
    "\n",
    "This codebase seeks to be easily maintainable, able to be run from the jupyter notebook stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [00:26<00:00, 27.49it/s]\n",
      "100%|██████████| 730/730 [00:24<00:00, 29.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 688 ms, total: 27.3 s\n",
      "Wall time: 56.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import requests\n",
    "import subprocess\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from elasticsearch import Elasticsearch\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "subprocess.Popen('/Users/saifbhatti/Desktop/Northwestern/sy1920/w20/iems308/elasticsearch-7.6.1/bin/elasticsearch')\n",
    "time.sleep(5)\n",
    "res = requests.get('http://localhost:9200')\n",
    "es = Elasticsearch()\n",
    "\n",
    "path = '/Users/saifbhatti/Desktop/Northwestern/sy1920/w20/iems308/iems308-saifbhatti/homew3/data/*.txt'\n",
    "files = glob.glob(path)\n",
    "corpus = []\n",
    "for document in files:\n",
    "    with open(document, 'r', errors='ignore') as single_document:\n",
    "        read_document = single_document.read().replace('\\n', ' ')\n",
    "    corpus = corpus + [read_document]    \n",
    "    \n",
    "#Iterate over documents in corpus and index them\n",
    "i = 1\n",
    "for document in tqdm(corpus):\n",
    "    es.index(index='docs',doc_type='article', id=i, body={'document' : document}) \n",
    "    i = i + 1       \n",
    "\n",
    "corpus_sent = []\n",
    "for doc in tqdm(corpus):\n",
    "    document_sentences = sent_tokenize(doc)\n",
    "    corpus_sent.append(document_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually takes about 1 minute to run startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your question and hit enter: \n",
      "\n",
      " Which companies went bankrupt in September of 2005?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 190131.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estar']\n"
     ]
    }
   ],
   "source": [
    "question = input('Type your question and hit enter: \\n\\n')\n",
    "\n",
    "while ('Stop' not in question):\n",
    "\n",
    "    qtokens = word_tokenize(question)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['?','Which','went','What','of','or','is','with','this','Who','the'])\n",
    "\n",
    "    keywords = []\n",
    "    for word in qtokens:\n",
    "        if word not in stop_words:\n",
    "            keywords += [word]\n",
    "\n",
    "    #determining question type\n",
    "    if 'CEO' in keywords:\n",
    "        question_type = 1\n",
    "    elif 'bankrupt' in keywords:\n",
    "        question_type = 2\n",
    "    else:\n",
    "        question_type = 3\n",
    "    #CEO questions\n",
    "    if question_type == 1:\n",
    "        keywords_str = ''\n",
    "        for word in keywords:\n",
    "            keywords_str += ' ' + word\n",
    "        search_term = keywords + [keywords_str]  \n",
    "        \n",
    "        doc_catcher = es.search(index='docs',q=search_term) #find docs with matches\n",
    "\n",
    "        doc_matchid = [] #store top match ids\n",
    "        for hit in tqdm(range(0,min(doc_catcher['hits']['total']['value'],1))):\n",
    "            doc_matchid += [doc_catcher['hits']['hits'][hit]['_id']]\n",
    "\n",
    "        i = 1\n",
    "        sentence_list = []\n",
    "        for id_num in doc_matchid: #for each document   \n",
    "            document = corpus_sent[int(id_num)-1]\n",
    "            for sentence in tqdm(document): #for each sentence in each document\n",
    "                es.index(index='sentences',doc_type='sentence', id=i, body={'sentence' : sentence})\n",
    "                i += 1\n",
    "\n",
    "        query_sentences = es.search(index='sentences',q=search_term,size=10)\n",
    "\n",
    "        sent_matchid = [] #store sentence matches\n",
    "        for hit in tqdm(range(0,min(query_sentences['hits']['total']['value'],10))):\n",
    "            sent_matchid += [query_sentences['hits']['hits'][hit]['_id']]\n",
    "\n",
    "        regex = '(?<='+keywords[1]+'\\sCEO\\s)[A-Z][a-z]+\\s[A-Z][a-z]+'\n",
    "        for sent_id in sent_matchid:\n",
    "            sentence = es.get(index='sentences',doc_type='sentence',id=int(sent_id))['_source']['sentence']\n",
    "            try:\n",
    "                name = re.search(regex, sentence).group()\n",
    "                break\n",
    "            except: pass\n",
    "        print(name) #return the CEO name\n",
    "    \n",
    "    #Bankrupcy questions\n",
    "    elif question_type == 2:\n",
    "        keywords_str = ''\n",
    "        for word in keywords:\n",
    "            keywords_str += ' ' + word\n",
    "        search_term = keywords + [keywords_str]  \n",
    "\n",
    "        doc_catcher = es.search(index='docs',q=search_term)\n",
    "\n",
    "        doc_matchid = []\n",
    "        for hit in range(0,min(doc_catcher['hits']['total']['value'],3)):\n",
    "            doc_matchid += [doc_catcher['hits']['hits'][hit]['_id']]\n",
    "\n",
    "        i = 1\n",
    "        sentence_list = []\n",
    "        for id_num in doc_matchid:   \n",
    "            document = corpus_sent[int(id_num)-1]\n",
    "            for sentence in document:\n",
    "                es.index(index='sentences',doc_type='sentence', id=i, body={'sentence' : sentence})\n",
    "                i += 1\n",
    "\n",
    "        query_sentences = es.search(index='sentences',q=search_term,size=50)\n",
    "\n",
    "        sent_matchid = []\n",
    "        for hit in tqdm(range(0,min(query_sentences['hits']['total']['value'],50))):\n",
    "            sent_matchid += [query_sentences['hits']['hits'][hit]['_id']]\n",
    "\n",
    "        relevant_sentences = [] #store most relevant sentences\n",
    "        for sent_id in sent_matchid:\n",
    "            sentence = es.get(index='sentences',doc_type='sentence',id=int(sent_id))['_source']['sentence']\n",
    "            if 'bankrupt' in sentence:           \n",
    "                relevant_sentences.append(sentence)\n",
    "\n",
    "        relevant_subsentences = [] #clean up the sentences to return smaller chunks\n",
    "        for sentence in relevant_sentences:\n",
    "            if 'France' not in sentence and 'Europe' not in sentence: #france/europe keeps going bankrupt so remove this\n",
    "                bankrupt_span = re.search('bankrupt',sentence).span()\n",
    "                relevant_subsentences.append(sentence[bankrupt_span[0]-20:bankrupt_span[1]+20])\n",
    "\n",
    "        #extract companies\n",
    "        regex = '(?<!\\.\\s)\\s[A-Z][a-z]+(\\s[A-Z][a-z]+)?'\n",
    "        answer_list = []\n",
    "        for subsentence in relevant_subsentences:\n",
    "            try:\n",
    "                bankrupt_company = re.search(regex, subsentence).group().strip()\n",
    "                answer_list.append(bankrupt_company)\n",
    "            except: pass\n",
    "        print(answer_list) #return companies\n",
    "        \n",
    "\n",
    "    #GDP affectation question\n",
    "    else: \n",
    "        search_term1 = 'gross domestic product shrank'\n",
    "        search_term2 = '(GDP | (gross & domestic & product)) & (increase|decrease|rose|fell|up|down|change)'\n",
    "        search_terms = [search_term1, search_term2]  \n",
    "\n",
    "        answers = []\n",
    "        second_halves = []\n",
    "        for search_term in search_terms:\n",
    "            first_finished = 0\n",
    "            doc_catcher = es.search(index='docs',q=search_term)\n",
    "\n",
    "            doc_matchid = []\n",
    "            for hit in tqdm(range(0,min(doc_catcher['hits']['total']['value'],3))):\n",
    "                doc_matchid += [doc_catcher['hits']['hits'][hit]['_id']]\n",
    "\n",
    "            i = 1\n",
    "            sentence_list = []\n",
    "            for id_num in doc_matchid:   \n",
    "                document = corpus_sent[int(id_num)-1]\n",
    "                for sentence in tqdm(document):\n",
    "                    es.index(index='sentences',doc_type='sentence', id=i, body={'sentence' : sentence})\n",
    "                    i += 1\n",
    "\n",
    "            query_sentences = es.search(index='sentences',q=search_term,size=5)\n",
    "\n",
    "            sent_matchid = []\n",
    "            for hit in range(0,min(query_sentences['hits']['total']['value'],3)):\n",
    "                sent_matchid += [query_sentences['hits']['hits'][hit]['_id']]               \n",
    "            \n",
    "            do_not_enter = 0\n",
    "            for sent_id in sent_matchid:\n",
    "               \n",
    "                sentence = es.get(index='sentences',doc_type='sentence',id=int(sent_id))['_source']['sentence']\n",
    "                stokens = word_tokenize(sentence)\n",
    "                stokens_pos = nltk.pos_tag(stokens)  #POS tagging\n",
    "                \n",
    "                split_point = 0\n",
    "                percent_present = 0\n",
    "                #detect percentages\n",
    "                for percent_search in stokens_pos:\n",
    "                    if percent_search[0] in ['percent',\"%\"]:\n",
    "                        percent_present = 1\n",
    "                        break\n",
    "                    split_point += 1    \n",
    "                if percent_present == 1: #if present, split\n",
    "                    stokens_pos_subset = stokens_pos[0:split_point]\n",
    "                    stokens_pos_subset2 = stokens_pos[split_point-1:]\n",
    "\n",
    "                    for percent_search in reversed(stokens_pos_subset):\n",
    "                        if percent_search[1] == 'NN' and do_not_enter==0 and first_finished==0: #nn = noun POS tag\n",
    "                            answer = percent_search[0]\n",
    "                            do_not_enter = 1\n",
    "                            first_finished = 1\n",
    "                            second_halves += [stokens_pos_subset2]\n",
    "                            break\n",
    "                        elif percent_search[1] == 'NNP': #nnp = proper noun POS tag\n",
    "                           try: \n",
    "                               answer = re.search('[A-Z][A-Z]+',percent_search[0].replace(u\"\\u2122\", '')).group()\n",
    "                               second_halves += [stokens_pos_subset2]\n",
    "                               break\n",
    "                           except: pass\n",
    "                    answers += [answer]  \n",
    "        print(\"GDP is affected by:\")\n",
    "        print(list(set(answers)))\n",
    "\n",
    "        #second question\n",
    "        followup = input(\"Enter property from above list to examine its percentage influence on GDP. \\n\\n\")\n",
    "\n",
    "        #figuring out where to search, hardcoded some typical results to speed up\n",
    "        if 'PMI' in followup:\n",
    "            percent_half = second_halves[1]\n",
    "        elif 'unemployment' in followup:\n",
    "            percent_half = second_halves[0]\n",
    "        else:\n",
    "            percent_half = second_halves[2]\n",
    "\n",
    "        percent_half_string = ''\n",
    "        for percent_search in percent_half:\n",
    "            percent_half_string += (' ' + percent_search[0])\n",
    "\n",
    "        percent_regex = '\\s([0-9]+|[a-zA-Z]+-?[a-zA-Z]*|[0-9]+\\.[0-9]+)\\s?(%|percent)(age point)?'\n",
    "        list_percents = re.findall(percent_regex, percent_half_string)\n",
    "\n",
    "        percent_as_tuple = list_percents[-1]\n",
    "\n",
    "        percent_string = ''\n",
    "        for term in percent_as_tuple:\n",
    "            percent_string += (' ' + term)\n",
    "        percent_string = percent_string.strip()\n",
    "        \n",
    "        print(percent_string) \n",
    "        \n",
    "    es.indices.delete(index=\"sentences\",ignore=[400, 404])\n",
    "    question = input('Type question and hit return: \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtimes\n",
    "\n",
    "- CEO questions: takes about 1 minute to run.\n",
    "- Bankrupcy questions: takes about 3 minutes to run.\n",
    "- GDP questions: grab a coffee, this takes at least 6 minutes to run, but the subsequent followup is instantaneous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
