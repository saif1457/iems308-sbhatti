{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEMS 308 - Data Science, Named Entity Recognition Engine\n",
    "\n",
    "1. The data can be found at the following [link](www.google.com).\n",
    "2. **Build a text pre-processing engine**\n",
    "3. Build a simple AI computation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "directory = 'data/' #directory to search for data\n",
    "filenames = [] #new list to hold filenames\n",
    "dates = [] #new list to hold date keys\n",
    "\n",
    "#traverse directory and import all relevant data\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"): #just double check correct filetype\n",
    "        url = os.path.join(directory,filename)\n",
    "        dates.append(filename.strip('.txt'))\n",
    "        filenames.append(url)\n",
    "    else: continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.sort() #organise into date order\n",
    "filenames[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import string\n",
    "printable = set(string.printable)\n",
    "\n",
    "content = []\n",
    "for filename in tqdm(filenames):\n",
    "    with open(filename,encoding='utf8',errors='ignore') as f:\n",
    "        document = f.read()\n",
    "        document = ''.join(filter(lambda x: x in printable, document))\n",
    "        document = document.replace('\\n', ' ')\n",
    "        content.append(document)\n",
    "f.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess `.csv` data\n",
    "\n",
    "Extract training set from `csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#extract all in one step\n",
    "\n",
    "# ceo_labels = ['','2','full']\n",
    "cents_labels = ['cent']\n",
    "corps_labels = ['first']\n",
    "\n",
    "training_ceo = pd.read_csv('/Users/saifbhatti/Desktop/Northwestern/sy1920/w20/iems308/iems308-saifbhatti/homew3/trained/ceo.csv',header=None,encoding='utf-8')\n",
    "training_cents = pd.read_csv('/Users/saifbhatti/Desktop/Northwestern/sy1920/w20/iems308/iems308-saifbhatti/homew3/trained/percentage.csv',names=cents_labels,header=None,encoding='utf-8')\n",
    "training_corp = pd.read_csv('/Users/saifbhatti/Desktop/Northwestern/sy1920/w20/iems308/iems308-saifbhatti/homew3/trained/companies.csv',header=None,names=corps_labels,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ceo[2] = [' '.join(s.split()) for s in training_ceo[2]]\n",
    "#convert all to list\n",
    "train_ceo = list(set(training_ceo[2].tolist()))\n",
    "train_corp = training_corp['first'].to_list()\n",
    "train_cent = training_cents['cent'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Named-Entity-Recognition Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['tagger','parser'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "doc = nlp(content[0])\n",
    "# for token in doc.ents:\n",
    "#     print(token.text, token.start_char, token.end_char,token.label_)\n",
    "#     print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`processed` holds text files which have been parsed by `spaCy`'s NER recognition, converting their type from `str` to `spacy.Doc.Docs`.\n",
    "\n",
    "#####Â Warning: `processed` runtime is approx 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time processed = [nlp(article) for article in tqdm(content[0:len(content)])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to not waste time, `processed` can be pickled and saved to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff=[]\n",
    "# for i in range(50):\n",
    "#     temp = matching_engine_3000(nlp(people_found[i]),\n",
    "#                                 'PERSON')\n",
    "#     if len(temp)>1:\n",
    "#         for j in range(len(temp)):\n",
    "#             print(temp[j])\n",
    "#             people_found[i]\n",
    "# #     stuff.extend(matching_engine_3000(nlp(filter_sentences(processed[0],\"PERSON\")[i]),'PERSON'))\n",
    "    \n",
    "# # for i in hello:\n",
    "# # #     print(i)\n",
    "# #     if len(i)>1:\n",
    "# #         for j in range(len(i)):\n",
    "# #             print(i[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_sentences(processed[0],'PERSON') #list\n",
    "# '''\n",
    "# within the list of sentences with entities,\n",
    "# look through all sentences, and check if there are more than 1 entity\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "1. `pickle_me_hearties` pickles files.\n",
    "2. `return_me_hearties` unpickles files.\n",
    "3. `matching_engine_3000` takes `nlp` `processed` (which are `spaCy.Doc.Doc`s, and matches given entity type.\n",
    "4. `filter sentences` takes `spaCy.Doc.Doc` and returns the `entity` present in the doc.\n",
    "5. `match` Iterates through list of substrings to see if any are in a sentence.\n",
    "6. `company_word` runs `re.search` on the sentence to detect presence of common Company words.\n",
    "7. `num_capitals` returns number of capitals in the sentence.\n",
    "8. `num_words` returns number of words in the sentence.\n",
    "9. `split_entity` replicates the sentence if multiple entities (of the same type) are found in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pickle_me_hearties(pval,pstart,pend):\n",
    "    '''\n",
    "    pickle_me_hearties pickles files.\n",
    "    \n",
    "    pval is the file iterator\n",
    "    pstart is start slice\n",
    "    pend is end slice\n",
    "    '''\n",
    "    with open('processed'+str(pval)+'.pkl','wb') as pf:\n",
    "        pickle.dump(processed[pstart:pend],pf,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_me_hearties(plist):\n",
    "    '''\n",
    "    return_me_hearties unpickles files.\n",
    "    \n",
    "    plist is the list of file iterators\n",
    "    '''\n",
    "    full_store=[]\n",
    "    for i in tqdm(plist):\n",
    "        with open('processed'+str(i)+'.pkl','rb') as pf:\n",
    "            var = pickle.load(pf)\n",
    "            full_store += var\n",
    "    return full_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time pickle_me_hearties(1,0,1)\n",
    "# %time pickle_me_hearties(2,151,300)\n",
    "# %time pickle_me_hearties(3,301,450)\n",
    "# %time pickle_me_hearties(4,451,600)\n",
    "# %time pickle_me_hearties(5,600,729)\n",
    "# %time new = return_me_hearties([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_engine_3000(values,match):\n",
    "    '''\n",
    "    values is the nlp processed docs\n",
    "    match is the entity type to match e.g. PERSON\n",
    "    '''\n",
    "    engine_results = [ent.text for ent in values.ents if ent.label_ == match]\n",
    "    return engine_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences(article, entity):\n",
    "    sentences = [sent.text for sent in article.sents]\n",
    "    entities = [ent.text for ent in article.ents if ent.label_ == entity]\n",
    "    sent_w_ent = []\n",
    "    for s in sentences:\n",
    "        if any(ent in s for ent in entities):\n",
    "            sent_w_ent.append(s)\n",
    "    \n",
    "    return sent_w_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(sentence, label_lst):\n",
    "    '''\n",
    "    Iterates through list of substrings to see if any are in a sentence.\n",
    "    Used for labeling the data as positive and negative samples generally. \n",
    "    '''\n",
    "    if any(lbl in sentence for lbl in label_lst):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_word(sentence):\n",
    "    if (bool(re.search(r'(Advisors|Partner|LP|Associate|Co|Group|LTD|AirLL|Management|Capital)',sentence))):\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_capitals(phrase):\n",
    "    cap_count = 0\n",
    "    for letter in phrase:\n",
    "        if letter.isupper():\n",
    "            cap_count = cap_count + 1\n",
    "    return(cap_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words(phrase):\n",
    "    return len(phrase.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Percentages\n",
    "\n",
    "Due to `spaCy`'s phenomenal NER capabilities, it's possible to directly extract all percentages super easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "takehome_percentages =[]\n",
    "for i in processed:\n",
    "    takehome_percentages.extend(matching_engine_3000(i,\"PERCENT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takehome_percentages=list(set(takehome_percentages))\n",
    "takehome_percent = pd.DataFrame(takehome_percentages)\n",
    "takehome_percent.to_csv('takehome_percent.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing CEOs\n",
    "\n",
    "Using `spaCy`'s NER capabilities, we have the ability to extract all `PERSON` entities, and run Machine Learning steps on this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_person_list = []\n",
    "for i in tqdm(range(len(processed))):\n",
    "    people = matching_engine_3000(processed[i],\"PERSON\")\n",
    "    full_person_list.append(people)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_sentences(processed[0],\"PERSON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "people_found = [] #people_found just holds a list of sentences if entity is present\n",
    "#we want to append new sentences if there are more than 1 entity in a sentence\n",
    "for i in tqdm(range(len(processed))):\n",
    "    people_found.extend(filter_sentences(processed[i],\"PERSON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_in = people_found\n",
    "# len(people_found)\n",
    "# len(find_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(275,277):\n",
    "#     print('sentence {}: {}'.format(i,people_found[i]))\n",
    "#     temp = matching_engine_3000(nlp(people_found[i]),'PERSON')\n",
    "#     if len(temp)>0:\n",
    "#         print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# finder=[]\n",
    "# for i in range(1):\n",
    "#     index_holder=[]\n",
    "#     print('sentence {}: {}'.format(i,people_found[i]))\n",
    "#     temp = matching_engine_3000(nlp(people_found[i]),'PERSON')\n",
    "#     if len(temp)>1:\n",
    "#         j = len(temp)\n",
    "# #         print(temp)\n",
    "# #         print(type(temp))\n",
    "#         for j in range(len(temp)):\n",
    "#             print('REPLACE LOOP')\n",
    "# #             print(temp[j])\n",
    "# #             print(re.sub(temp[j], '', people_found[i]))\n",
    "#             index_holder.append(people_found[i].index(temp[j]))\n",
    "#             print(index_holder)\n",
    "#             people_found[i] = (re.sub(temp[j], '', people_found[i]))\n",
    "#             print(people_found[i])\n",
    "#         for j in range(len(temp)):\n",
    "#             print('ADD LOOP')\n",
    "# #             print(temp[j])\n",
    "# #             print(re.sub(temp[j], '', people_found[i]))\n",
    "# #             people_found[i] = people_found[i][:index_holder[j]] + temp[j] + people_found[i][index_holder[j]:]\n",
    "#             people_found[i] = insert_str(people_found[i],temp[j],index_holder[j])\n",
    "#             print(people_found[i])\n",
    "# #             s[:4] + '-' + s[4:]\n",
    "# #     print(people_found[i])\n",
    "\n",
    "\n",
    "# #             print(new_temp)\n",
    "# #         print(people_found[i])\n",
    "# #     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_str(string, str_to_insert, index):\n",
    "# #     return string[:index] + str_to_insert + string[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# finder=[]\n",
    "# for i in tqdm(range(len(find_in))):\n",
    "#     index_holder=[]\n",
    "# #     print('----------')\n",
    "# #     print('sentence {}: {}'.format(i,find_in[i]))\n",
    "#     temp = matching_engine_3000(nlp(find_in[i]),'PERSON')\n",
    "#     if len(temp)>1:\n",
    "#         j = len(temp)\n",
    "# #         print(temp)\n",
    "# #         print(type(temp))\n",
    "#         for j in range(len(temp)):\n",
    "# #             print('REPLACE LOOP')\n",
    "#             index_holder.append(find_in[i].index(temp[j]))\n",
    "# #             print(index_holder)\n",
    "#             find_in[i] = (re.sub(temp[j], '', find_in[i],1))\n",
    "# #             print(find_in[i])\n",
    "#         for j in range(len(temp)):\n",
    "# #             print('ADD LOOP')\n",
    "#             find_in[i] = insert_str(find_in[i],temp[j],index_holder[j])\n",
    "#             finder.extend(find_in[i])\n",
    "# #             print(find_in[i])\n",
    "#             find_in[i] = (re.sub(temp[j], '', find_in[i]))\n",
    "#     elif len(temp)<1:\n",
    "#         finder.append(find_in[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_df = pd.DataFrame({'text_persons':people_found})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ceo_df['num_capitals'] = ceo_df['text_persons'].apply(lambda x: (num_capitals(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ceo_df['ceo_label'] = ceo_df['text_persons'].apply(lambda x: (match(x,train_ceo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ceo_df['num_words'] = ceo_df['text_persons'].apply(lambda x: (num_words(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_df.to_csv('ceo_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Companies\n",
    "\n",
    "Using `spaCy`'s NER capabilities, we have the ability to extract all `ORG` entities, and run Machine Learning steps on this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_company_list = []\n",
    "for i in tqdm(range(len(processed))):\n",
    "    companies = matching_engine_3000(processed[i],\"ORG\")\n",
    "    full_company_list.extend(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organisations_found = []\n",
    "for i in tqdm(range(len(processed))):\n",
    "    organisations_found.extend(filter_sentences(processed[i],\"ORG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.DataFrame({'text_organisations':organisations_found})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time company_df['num_words'] = company_df['text_organisations'].apply(lambda x: (num_words(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time company_df['num_capitals'] = company_df['text_organisations'].apply(lambda x: (num_capitals(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time company_df['company_nearby'] = company_df['text_organisations'].apply(lambda x: company_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time company_df['company_label'] = company_df['text_organisations'].apply(lambda x: (match(x,train_corp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df[['company_nearby','company_label','num_capitals','num_words']].describe()\n",
    "#the results from this indicate that roughly 7% of the sentences contain matches\n",
    "#given almost 350k sentences containing persons, 7% would be 24,500 sentences containing matched ceos\n",
    "#this seems reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df.to_csv('company_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
